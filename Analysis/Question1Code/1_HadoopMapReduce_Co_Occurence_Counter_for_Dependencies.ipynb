{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Joey-mi/CPSC572---Electron-Packages/blob/main/HadoopMapReduce_Co_Occurence_Counter_for_Dependencies.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZXXWIIZK7fi",
        "outputId": "5c4d6c09-7f4d-46eb-db65-166eed81c629"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deleted /inputs/nonDevDependencies\n",
            "2024-03-26 06:22:18,277 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
            "2024-03-26 06:22:18,544 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n"
          ]
        }
      ],
      "source": [
        "# Create the input and output files on hadoop DFS then put our test data in\n",
        "!hadoop fs -mkdir -p /outputs\n",
        "!hadoop fs -mkdir -p /inputs\n",
        "!hdfs dfs -rm -r /inputs/nonDevDependencies\n",
        "!hadoop fs -put nonDevDependencies /inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tf37hP4vK7fi",
        "outputId": "9f421c20-dd04-4d9f-c1ff-4184da6b283d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting mapper3.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile mapper3.py\n",
        "#!/opt/bitnami/python/bin/python\n",
        "# -*-coding:utf-8 -*\n",
        "\n",
        "import sys\n",
        "import os\n",
        "\n",
        "for line in sys.stdin: # reads from stdin\n",
        "\n",
        "    # Convert string to list\n",
        "    array = list(map(str.strip, line.strip('][').replace('\"', '').split(',')))\n",
        "\n",
        "    # Clean up strings in list\n",
        "    array = list(map(lambda x: x.replace(\"'\", \"\").replace('\"', '').replace(\"@\", \"\").replace('[', '').replace(']', ''), array))\n",
        "\n",
        "    n = len(array)\n",
        "\n",
        "    # Since arrays are already sorted, we iterate through them in O(n^2) to get all possible pairs\n",
        "    for i in range(n):\n",
        "        for j in range(i+1, n):\n",
        "            print(array[i][0] + \",\" + array[i] + \",\" + array[j])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXeY9JFNK7fj",
        "outputId": "b577c445-a200-47d2-81f2-f6c3cdff4c29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting reducer3.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile reducer3.py\n",
        "#!/opt/bitnami/python/bin/python\n",
        "# -*-coding:utf-8 -*\n",
        "\n",
        "import sys\n",
        "import os\n",
        "\n",
        "pair = None\n",
        "pairedCount = 0\n",
        "\n",
        "for line in sys.stdin:\n",
        "\n",
        "    intKv = line.strip().split(\",\") # item1[0],item1,item2\n",
        "    # Pass if we get an empty string\n",
        "    if len(intKv) != 3:\n",
        "        continue\n",
        "\n",
        "    parsedLine = intKv[1].strip()+\"@\"+intKv[2].strip()\n",
        "    kv = [parsedLine, \"1\"] # item1,item2<\\t>count\n",
        "\n",
        "\n",
        "    # When it's a different item pair, we will store the entry\n",
        "    if kv[0] != pair:\n",
        "        if pair is not None:\n",
        "            if pairedCount > 1:\n",
        "                print(f\"{pair}, {pairedCount}\")\n",
        "\n",
        "        # Reset the variables\n",
        "        pair = kv[0]\n",
        "        pairedCount = 1\n",
        "    # If it's the same item pair, then we increment the count\n",
        "    else:\n",
        "        pairedCount += 1\n",
        "\n",
        "# Output the last pair at the end\n",
        "if pair is not None:\n",
        "    if pairedCount > 1:\n",
        "        print(f\"{pair}, {pairedCount}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjJwu4_nK7fj",
        "outputId": "533e649c-82b6-48c3-cfb3-83b346b043b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deleted /outputs/project572\n",
            "2024-03-26 06:37:20,490 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [/training/Assignment2/mapper3.py, /training/Assignment2/reducer3.py, /tmp/hadoop-unjar1752999774726923773/] [] /tmp/streamjob3308468320850206538.jar tmpDir=null\n",
            "2024-03-26 06:37:23,191 INFO client.RMProxy: Connecting to ResourceManager at resourcemanager/172.18.0.3:8032\n",
            "2024-03-26 06:37:23,619 INFO client.AHSProxy: Connecting to Application History server at historyserver/172.18.0.5:10200\n",
            "2024-03-26 06:37:23,701 INFO client.RMProxy: Connecting to ResourceManager at resourcemanager/172.18.0.3:8032\n",
            "2024-03-26 06:37:23,702 INFO client.AHSProxy: Connecting to Application History server at historyserver/172.18.0.5:10200\n",
            "2024-03-26 06:37:24,172 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1711418519987_0005\n",
            "2024-03-26 06:37:24,449 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
            "2024-03-26 06:37:24,677 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
            "2024-03-26 06:37:24,742 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
            "2024-03-26 06:37:24,911 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2024-03-26 06:37:24,982 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
            "2024-03-26 06:37:25,015 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
            "2024-03-26 06:37:25,031 INFO mapreduce.JobSubmitter: number of splits:2\n",
            "2024-03-26 06:37:25,117 INFO Configuration.deprecation: mapred.text.key.partitioner.options is deprecated. Instead, use mapreduce.partition.keypartitioner.options\n",
            "2024-03-26 06:37:25,117 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
            "2024-03-26 06:37:25,118 INFO Configuration.deprecation: map.output.key.field.separator is deprecated. Instead, use mapreduce.map.output.key.field.separator\n",
            "2024-03-26 06:37:25,322 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
            "2024-03-26 06:37:25,810 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1711418519987_0005\n",
            "2024-03-26 06:37:25,810 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-03-26 06:37:26,223 INFO conf.Configuration: resource-types.xml not found\n",
            "2024-03-26 06:37:26,223 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
            "2024-03-26 06:37:27,002 INFO impl.YarnClientImpl: Submitted application application_1711418519987_0005\n",
            "2024-03-26 06:37:27,083 INFO mapreduce.Job: The url to track the job: http://resourcemanager:8088/proxy/application_1711418519987_0005/\n",
            "2024-03-26 06:37:27,086 INFO mapreduce.Job: Running job: job_1711418519987_0005\n",
            "2024-03-26 06:37:39,526 INFO mapreduce.Job: Job job_1711418519987_0005 running in uber mode : false\n",
            "2024-03-26 06:37:39,530 INFO mapreduce.Job:  map 0% reduce 0%\n",
            "2024-03-26 06:37:55,898 INFO mapreduce.Job:  map 50% reduce 0%\n",
            "2024-03-26 06:37:56,909 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2024-03-26 06:38:04,016 INFO mapreduce.Job:  map 100% reduce 4%\n",
            "2024-03-26 06:38:10,083 INFO mapreduce.Job:  map 100% reduce 8%\n",
            "2024-03-26 06:38:17,178 INFO mapreduce.Job:  map 100% reduce 12%\n",
            "2024-03-26 06:38:23,275 INFO mapreduce.Job:  map 100% reduce 15%\n",
            "2024-03-26 06:38:30,343 INFO mapreduce.Job:  map 100% reduce 19%\n",
            "2024-03-26 06:38:37,439 INFO mapreduce.Job:  map 100% reduce 23%\n",
            "2024-03-26 06:38:45,525 INFO mapreduce.Job:  map 100% reduce 27%\n",
            "2024-03-26 06:38:52,605 INFO mapreduce.Job:  map 100% reduce 31%\n",
            "2024-03-26 06:38:59,687 INFO mapreduce.Job:  map 100% reduce 35%\n",
            "2024-03-26 06:39:06,761 INFO mapreduce.Job:  map 100% reduce 38%\n",
            "2024-03-26 06:39:13,835 INFO mapreduce.Job:  map 100% reduce 42%\n",
            "2024-03-26 06:39:19,936 INFO mapreduce.Job:  map 100% reduce 46%\n",
            "2024-03-26 06:39:26,990 INFO mapreduce.Job:  map 100% reduce 50%\n",
            "2024-03-26 06:39:34,043 INFO mapreduce.Job:  map 100% reduce 54%\n",
            "2024-03-26 06:39:41,096 INFO mapreduce.Job:  map 100% reduce 58%\n",
            "2024-03-26 06:39:48,158 INFO mapreduce.Job:  map 100% reduce 62%\n",
            "2024-03-26 06:39:54,209 INFO mapreduce.Job:  map 100% reduce 65%\n",
            "2024-03-26 06:40:01,256 INFO mapreduce.Job:  map 100% reduce 69%\n",
            "2024-03-26 06:40:07,347 INFO mapreduce.Job:  map 100% reduce 73%\n",
            "2024-03-26 06:40:13,388 INFO mapreduce.Job:  map 100% reduce 77%\n",
            "2024-03-26 06:40:20,446 INFO mapreduce.Job:  map 100% reduce 81%\n",
            "2024-03-26 06:40:27,503 INFO mapreduce.Job:  map 100% reduce 85%\n",
            "2024-03-26 06:40:34,558 INFO mapreduce.Job:  map 100% reduce 88%\n",
            "2024-03-26 06:40:42,616 INFO mapreduce.Job:  map 100% reduce 92%\n",
            "2024-03-26 06:40:49,682 INFO mapreduce.Job:  map 100% reduce 96%\n",
            "2024-03-26 06:40:56,734 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2024-03-26 06:40:56,744 INFO mapreduce.Job: Job job_1711418519987_0005 completed successfully\n",
            "2024-03-26 06:40:56,990 INFO mapreduce.Job: Counters: 56\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=5214978\n",
            "\t\tFILE: Number of bytes written=17351592\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\t\tHDFS: Number of bytes read=2220429\n",
            "\t\tHDFS: Number of bytes written=4072477\n",
            "\t\tHDFS: Number of read operations=136\n",
            "\t\tHDFS: Number of large read operations=0\n",
            "\t\tHDFS: Number of write operations=52\n",
            "\t\tHDFS: Number of bytes read erasure-coded=0\n",
            "\tJob Counters \n",
            "\t\tKilled map tasks=1\n",
            "\t\tKilled reduce tasks=1\n",
            "\t\tLaunched map tasks=2\n",
            "\t\tLaunched reduce tasks=26\n",
            "\t\tRack-local map tasks=2\n",
            "\t\tTotal time spent by all maps in occupied slots (ms)=100940\n",
            "\t\tTotal time spent by all reduces in occupied slots (ms)=1197696\n",
            "\t\tTotal time spent by all map tasks (ms)=25235\n",
            "\t\tTotal time spent by all reduce tasks (ms)=149712\n",
            "\t\tTotal vcore-milliseconds taken by all map tasks=25235\n",
            "\t\tTotal vcore-milliseconds taken by all reduce tasks=149712\n",
            "\t\tTotal megabyte-milliseconds taken by all map tasks=103362560\n",
            "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1226440704\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=11475\n",
            "\t\tMap output records=1219479\n",
            "\t\tMap output bytes=37082207\n",
            "\t\tMap output materialized bytes=5593732\n",
            "\t\tInput split bytes=248\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=686402\n",
            "\t\tReduce shuffle bytes=5593732\n",
            "\t\tReduce input records=1219479\n",
            "\t\tReduce output records=132154\n",
            "\t\tSpilled Records=2438958\n",
            "\t\tShuffled Maps =52\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=52\n",
            "\t\tGC time elapsed (ms)=3783\n",
            "\t\tCPU time spent (ms)=86020\n",
            "\t\tPhysical memory (bytes) snapshot=5777682432\n",
            "\t\tVirtual memory (bytes) snapshot=228073512960\n",
            "\t\tTotal committed heap usage (bytes)=5483528192\n",
            "\t\tPeak Map Physical memory (bytes)=523935744\n",
            "\t\tPeak Map Virtual memory (bytes)=5040635904\n",
            "\t\tPeak Reduce Physical memory (bytes)=219004928\n",
            "\t\tPeak Reduce Virtual memory (bytes)=8389033984\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=2220181\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=4072477\n",
            "2024-03-26 06:40:56,992 INFO streaming.StreamJob: Output directory: /outputs/project572\n"
          ]
        }
      ],
      "source": [
        "# Remove the output file if already present\n",
        "!hdfs dfs -rm -r /outputs/project572\n",
        "\n",
        "# For the KeyFieldBasedPartitioner, specify we have three fields separated by \",\" and use the first field to partition with option k1,1\n",
        "# Specify 26 reduce tasks for each letter of the alphabet\n",
        "!hadoop jar /opt/hadoop-3.2.1/share/hadoop/tools/lib/hadoop-streaming-3.2.1.jar \\\n",
        "    -D stream.map.output.field.separator=, \\\n",
        "    -D stream.num.map.output.key.fields=3 \\\n",
        "    -D map.output.key.field.separator=, \\\n",
        "    -D mapred.text.key.partitioner.options=-k1,1 \\\n",
        "    -D mapred.reduce.tasks=26 \\\n",
        "    -file $PWD/mapper3.py\\\n",
        "    -file $PWD/reducer3.py\\\n",
        "    -input /inputs/nonDevDependencies/noDevCoOccurenceLists.txt \\\n",
        "    -output /outputs/project572 \\\n",
        "    -mapper mapper3.py \\\n",
        "    -reducer reducer3.py \\\n",
        "    -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWPr1QQRK7fk",
        "outputId": "7a0c981a-c98d-44d3-9cf2-33edbe281459"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-03-26 06:41:02,490 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\r\n"
          ]
        }
      ],
      "source": [
        "# Get the output from HDFS to local store\n",
        "!hadoop fs -copyToLocal /outputs/project572 countsFolder\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}